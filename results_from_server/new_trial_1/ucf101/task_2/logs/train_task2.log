INFO:root:

Training for task 2 starting on: 08/12/2025, 23:14:37

INFO:root:The config is being used from file: config/train_configs/UCF101/train_task2_unfreeze_ft.yml
INFO:root:
Starting with calculating features and relations for distillation purposes.


INFO:root:Task 1: Loading checkpoint from Task 0 at model_save/new_trial_1/ucf101/task_0/model/checkpoints/final_model.pth
INFO:root:Loading weights from: model_save/new_trial_1/ucf101/task_0/model/checkpoints/final_model.pth
INFO:root:Task 1: Freezing all parameters except for the new components.
INFO:root:Model architecture prepared: 1 adapter(s), 2 classifier(s).
INFO:root:

DO NOT WORRY ABOUT THE CHECKPOINT LOADING. 
We're now loading the actual checkpoint from model_save/new_trial_1/ucf101/task_1/model/checkpoints/final_model_after_ft.pth
INFO:root:Loading weights from: model_save/new_trial_1/ucf101/task_1/model/checkpoints/final_model_after_ft.pth
INFO:root:Extracting features and relations for memory bank...
INFO:root:Saved Spatial Features: model_save/new_trial_1/bank/task_1_model/spatial_feats_task1.pt (torch.Size([528, 512]))
INFO:root:Saved Temporal Features: model_save/new_trial_1/bank/task_1_model/temporal_feats_task1.pt (torch.Size([528, 512]))
INFO:root:Saved Spatial Relations: model_save/new_trial_1/bank/task_1_model/spatial_relations_task1.json (torch.Size([528, 52]))
INFO:root:Saved Temporal Relations: model_save/new_trial_1/bank/task_1_model/temporal_relations_task1.json (torch.Size([528, 52]))
INFO:root:Old features and relations bank created successfully, now starting current task: 2.


INFO:root:Task 2: Loading checkpoint from Task 1 at model_save/new_trial_1/ucf101/task_1/model/checkpoints/final_model_after_ft.pth
INFO:root:Loading weights from: model_save/new_trial_1/ucf101/task_1/model/checkpoints/final_model_after_ft.pth
INFO:root:Task 2: Freezing all parameters except for the new components.
INFO:root:Model architecture prepared: 2 adapter(s), 3 classifier(s).
INFO:root:Model config: model_name: csta, dim: 512, num_heads: 8, num_layers: 8


INFO:root:| Training Epoch 0:: | | Average Loss: 4.9607, | | Average Accuracy: 0.0000, | | Avg Grad Norm: 96.5323, | | Samples: 528 | | CE Loss: 4.7238 | | Distill Loss: 0.4251 | | Lt, Ls Loss: 0.5640, 0.5899 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 0:: | | Average Loss: 4.0250, | | Average Accuracy: 0.0000, | | Samples: 87 | | CE Loss: 3.8113 | | Distill Loss: 0.3485 | | Lt, Ls Loss: 0.4741, 0.6020 | 
INFO:root:| Training Epoch 1:: | | Average Loss: 3.4671, | | Average Accuracy: 0.1042, | | Avg Grad Norm: 46.8426, | | Samples: 528 | | CE Loss: 3.2442 | | Distill Loss: 0.4078 | | Lt, Ls Loss: 0.5130, 0.5655 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 1:: | | Average Loss: 3.0235, | | Average Accuracy: 0.1034, | | Samples: 87 | | CE Loss: 2.7794 | | Distill Loss: 0.5209 | | Lt, Ls Loss: 0.5578, 0.5481 | 
INFO:root:| Training Epoch 2:: | | Average Loss: 2.6763, | | Average Accuracy: 0.2235, | | Avg Grad Norm: 48.6849, | | Samples: 528 | | CE Loss: 2.4121 | | Distill Loss: 0.7243 | | Lt, Ls Loss: 0.5197, 0.5173 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 2:: | | Average Loss: 2.3935, | | Average Accuracy: 0.2874, | | Samples: 87 | | CE Loss: 2.1171 | | Distill Loss: 0.9243 | | Lt, Ls Loss: 0.4814, 0.4375 | 
INFO:root:| Training Epoch 3:: | | Average Loss: 2.3922, | | Average Accuracy: 0.2784, | | Avg Grad Norm: 42.4082, | | Samples: 528 | | CE Loss: 2.0826 | | Distill Loss: 0.9979 | | Lt, Ls Loss: 0.5525, 0.5136 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 3:: | | Average Loss: 2.3121, | | Average Accuracy: 0.2299, | | Samples: 87 | | CE Loss: 2.0164 | | Distill Loss: 1.0852 | | Lt, Ls Loss: 0.4695, 0.4169 | 
INFO:root:| Training Epoch 4:: | | Average Loss: 2.3001, | | Average Accuracy: 0.2652, | | Avg Grad Norm: 42.5040, | | Samples: 528 | | CE Loss: 2.0077 | | Distill Loss: 1.1113 | | Lt, Ls Loss: 0.4248, 0.4133 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 4:: | | Average Loss: 2.3408, | | Average Accuracy: 0.3563, | | Samples: 87 | | CE Loss: 2.0044 | | Distill Loss: 1.1215 | | Lt, Ls Loss: 0.5877, 0.5335 | 
INFO:root:| Training Epoch 5:: | | Average Loss: 2.2868, | | Average Accuracy: 0.3277, | | Avg Grad Norm: 49.0518, | | Samples: 528 | | CE Loss: 1.9960 | | Distill Loss: 1.1374 | | Lt, Ls Loss: 0.4194, 0.3814 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 5:: | | Average Loss: 2.2114, | | Average Accuracy: 0.3448, | | Samples: 87 | | CE Loss: 1.8805 | | Distill Loss: 1.2755 | | Lt, Ls Loss: 0.4800, 0.4501 | 
INFO:root:| Training Epoch 6:: | | Average Loss: 2.2142, | | Average Accuracy: 0.3030, | | Avg Grad Norm: 61.8384, | | Samples: 528 | | CE Loss: 1.8498 | | Distill Loss: 1.3160 | | Lt, Ls Loss: 0.5476, 0.5661 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 6:: | | Average Loss: 2.2009, | | Average Accuracy: 0.2989, | | Samples: 87 | | CE Loss: 1.8349 | | Distill Loss: 1.3395 | | Lt, Ls Loss: 0.5349, 0.5656 | 
INFO:root:| Training Epoch 7:: | | Average Loss: 2.2924, | | Average Accuracy: 0.2803, | | Avg Grad Norm: 41.3443, | | Samples: 528 | | CE Loss: 1.9419 | | Distill Loss: 1.2616 | | Lt, Ls Loss: 0.5340, 0.5412 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 7:: | | Average Loss: 2.4249, | | Average Accuracy: 0.2299, | | Samples: 87 | | CE Loss: 2.1276 | | Distill Loss: 1.0617 | | Lt, Ls Loss: 0.4751, 0.4454 | 
INFO:root:| Training Epoch 8:: | | Average Loss: 2.3588, | | Average Accuracy: 0.2822, | | Avg Grad Norm: 17.7733, | | Samples: 528 | | CE Loss: 2.0275 | | Distill Loss: 1.1834 | | Lt, Ls Loss: 0.5085, 0.5168 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 8:: | | Average Loss: 2.1884, | | Average Accuracy: 0.3563, | | Samples: 87 | | CE Loss: 1.8343 | | Distill Loss: 1.3761 | | Lt, Ls Loss: 0.4671, 0.5174 | 
INFO:root:| Training Epoch 9:: | | Average Loss: 2.1297, | | Average Accuracy: 0.3182, | | Avg Grad Norm: 23.4776, | | Samples: 528 | | CE Loss: 1.7637 | | Distill Loss: 1.4323 | | Lt, Ls Loss: 0.4441, 0.5639 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 9:: | | Average Loss: 2.1038, | | Average Accuracy: 0.2644, | | Samples: 87 | | CE Loss: 1.7450 | | Distill Loss: 1.4330 | | Lt, Ls Loss: 0.4535, 0.5059 | 
INFO:root:| Training Epoch 10:: | | Average Loss: 2.1308, | | Average Accuracy: 0.3239, | | Avg Grad Norm: 24.5219, | | Samples: 528 | | CE Loss: 1.7614 | | Distill Loss: 1.3720 | | Lt, Ls Loss: 0.5020, 0.5889 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 10:: | | Average Loss: 2.1277, | | Average Accuracy: 0.3793, | | Samples: 87 | | CE Loss: 1.7706 | | Distill Loss: 1.3293 | | Lt, Ls Loss: 0.4596, 0.5913 | 
INFO:root:| Training Epoch 11:: | | Average Loss: 2.1012, | | Average Accuracy: 0.3996, | | Avg Grad Norm: 16.0531, | | Samples: 528 | | CE Loss: 1.7686 | | Distill Loss: 1.3131 | | Lt, Ls Loss: 0.4208, 0.4836 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 11:: | | Average Loss: 2.0849, | | Average Accuracy: 0.2989, | | Samples: 87 | | CE Loss: 1.7526 | | Distill Loss: 1.3563 | | Lt, Ls Loss: 0.4107, 0.4487 | 
INFO:root:| Training Epoch 12:: | | Average Loss: 2.0614, | | Average Accuracy: 0.3580, | | Avg Grad Norm: 42.1988, | | Samples: 528 | | CE Loss: 1.7163 | | Distill Loss: 1.4445 | | Lt, Ls Loss: 0.4198, 0.4365 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 12:: | | Average Loss: 2.0345, | | Average Accuracy: 0.3678, | | Samples: 87 | | CE Loss: 1.6809 | | Distill Loss: 1.6135 | | Lt, Ls Loss: 0.3679, 0.3756 | 
INFO:root:| Training Epoch 13:: | | Average Loss: 2.0529, | | Average Accuracy: 0.3561, | | Avg Grad Norm: 23.8867, | | Samples: 528 | | CE Loss: 1.6768 | | Distill Loss: 1.6857 | | Lt, Ls Loss: 0.4163, 0.4051 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 13:: | | Average Loss: 2.0716, | | Average Accuracy: 0.3448, | | Samples: 87 | | CE Loss: 1.6896 | | Distill Loss: 1.6934 | | Lt, Ls Loss: 0.4390, 0.4144 | 
INFO:root:| Training Epoch 14:: | | Average Loss: 2.0895, | | Average Accuracy: 0.4205, | | Avg Grad Norm: 23.3146, | | Samples: 528 | | CE Loss: 1.6958 | | Distill Loss: 1.7296 | | Lt, Ls Loss: 0.4501, 0.4446 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 14:: | | Average Loss: 2.1033, | | Average Accuracy: 0.4023, | | Samples: 87 | | CE Loss: 1.6916 | | Distill Loss: 1.7476 | | Lt, Ls Loss: 0.5171, 0.4801 | 
INFO:root:| Training Epoch 15:: | | Average Loss: 2.1209, | | Average Accuracy: 0.4564, | | Avg Grad Norm: 24.4718, | | Samples: 528 | | CE Loss: 1.7031 | | Distill Loss: 1.7175 | | Lt, Ls Loss: 0.5565, 0.5115 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 15:: | | Average Loss: 2.0853, | | Average Accuracy: 0.3908, | | Samples: 87 | | CE Loss: 1.6877 | | Distill Loss: 1.6871 | | Lt, Ls Loss: 0.5078, 0.4562 | 
INFO:root:| Training Epoch 16:: | | Average Loss: 2.0402, | | Average Accuracy: 0.3636, | | Avg Grad Norm: 21.1759, | | Samples: 528 | | CE Loss: 1.6473 | | Distill Loss: 1.7183 | | Lt, Ls Loss: 0.4453, 0.4552 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 16:: | | Average Loss: 2.0310, | | Average Accuracy: 0.3448, | | Samples: 87 | | CE Loss: 1.6697 | | Distill Loss: 1.7156 | | Lt, Ls Loss: 0.3642, 0.3292 | 
INFO:root:| Training Epoch 17:: | | Average Loss: 2.0951, | | Average Accuracy: 0.3617, | | Avg Grad Norm: 16.8113, | | Samples: 528 | | CE Loss: 1.7272 | | Distill Loss: 1.6409 | | Lt, Ls Loss: 0.4269, 0.3847 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 17:: | | Average Loss: 2.0774, | | Average Accuracy: 0.2989, | | Samples: 87 | | CE Loss: 1.7534 | | Distill Loss: 1.5454 | | Lt, Ls Loss: 0.3014, 0.3134 | 
INFO:root:| Training Epoch 18:: | | Average Loss: 2.0512, | | Average Accuracy: 0.4072, | | Avg Grad Norm: 12.8638, | | Samples: 528 | | CE Loss: 1.7025 | | Distill Loss: 1.5956 | | Lt, Ls Loss: 0.3306, 0.3986 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 18:: | | Average Loss: 2.0208, | | Average Accuracy: 0.3793, | | Samples: 87 | | CE Loss: 1.6620 | | Distill Loss: 1.6788 | | Lt, Ls Loss: 0.3201, 0.3930 | 
INFO:root:| Training Epoch 19:: | | Average Loss: 2.0007, | | Average Accuracy: 0.4223, | | Avg Grad Norm: 16.2686, | | Samples: 528 | | CE Loss: 1.6251 | | Distill Loss: 1.7596 | | Lt, Ls Loss: 0.3493, 0.3954 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 19:: | | Average Loss: 2.0022, | | Average Accuracy: 0.3563, | | Samples: 87 | | CE Loss: 1.6079 | | Distill Loss: 1.7873 | | Lt, Ls Loss: 0.4123, 0.4294 | 
INFO:root:| Training Epoch 20:: | | Average Loss: 1.9539, | | Average Accuracy: 0.4489, | | Avg Grad Norm: 41.5947, | | Samples: 528 | | CE Loss: 1.5724 | | Distill Loss: 1.8215 | | Lt, Ls Loss: 0.3499, 0.3719 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 20:: | | Average Loss: 1.9686, | | Average Accuracy: 0.4023, | | Samples: 87 | | CE Loss: 1.5920 | | Distill Loss: 1.7836 | | Lt, Ls Loss: 0.3580, 0.3686 | 
INFO:root:| Training Epoch 21:: | | Average Loss: 1.9444, | | Average Accuracy: 0.4621, | | Avg Grad Norm: 18.5704, | | Samples: 528 | | CE Loss: 1.5593 | | Distill Loss: 1.8193 | | Lt, Ls Loss: 0.3602, 0.3877 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 21:: | | Average Loss: 1.9519, | | Average Accuracy: 0.4138, | | Samples: 87 | | CE Loss: 1.5607 | | Distill Loss: 1.8117 | | Lt, Ls Loss: 0.3931, 0.4036 | 
INFO:root:| Training Epoch 22:: | | Average Loss: 1.8976, | | Average Accuracy: 0.4583, | | Avg Grad Norm: 16.9156, | | Samples: 528 | | CE Loss: 1.5307 | | Distill Loss: 1.8073 | | Lt, Ls Loss: 0.3029, 0.3356 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 22:: | | Average Loss: 1.9172, | | Average Accuracy: 0.3793, | | Samples: 87 | | CE Loss: 1.5611 | | Distill Loss: 1.7572 | | Lt, Ls Loss: 0.2801, 0.3365 | 
INFO:root:| Training Epoch 23:: | | Average Loss: 1.8907, | | Average Accuracy: 0.4508, | | Avg Grad Norm: 18.5094, | | Samples: 528 | | CE Loss: 1.5208 | | Distill Loss: 1.7262 | | Lt, Ls Loss: 0.3517, 0.3880 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 23:: | | Average Loss: 1.8937, | | Average Accuracy: 0.4138, | | Samples: 87 | | CE Loss: 1.5345 | | Distill Loss: 1.7068 | | Lt, Ls Loss: 0.3345, 0.3533 | 
INFO:root:| Training Epoch 24:: | | Average Loss: 1.8681, | | Average Accuracy: 0.4848, | | Avg Grad Norm: 30.6763, | | Samples: 528 | | CE Loss: 1.4978 | | Distill Loss: 1.6969 | | Lt, Ls Loss: 0.3773, 0.3941 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 24:: | | Average Loss: 1.8662, | | Average Accuracy: 0.4713, | | Samples: 87 | | CE Loss: 1.5040 | | Distill Loss: 1.6822 | | Lt, Ls Loss: 0.3618, 0.3706 | 
INFO:root:| Training Epoch 25:: | | Average Loss: 1.8190, | | Average Accuracy: 0.5568, | | Avg Grad Norm: 16.8866, | | Samples: 528 | | CE Loss: 1.4503 | | Distill Loss: 1.7105 | | Lt, Ls Loss: 0.3808, 0.3670 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 25:: | | Average Loss: 1.8472, | | Average Accuracy: 0.4943, | | Samples: 87 | | CE Loss: 1.4573 | | Distill Loss: 1.7555 | | Lt, Ls Loss: 0.4112, 0.4329 | 
INFO:root:| Training Epoch 26:: | | Average Loss: 1.7886, | | Average Accuracy: 0.5322, | | Avg Grad Norm: 18.4433, | | Samples: 528 | | CE Loss: 1.4268 | | Distill Loss: 1.7346 | | Lt, Ls Loss: 0.3332, 0.3447 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 26:: | | Average Loss: 1.8113, | | Average Accuracy: 0.5057, | | Samples: 87 | | CE Loss: 1.4447 | | Distill Loss: 1.7251 | | Lt, Ls Loss: 0.3351, 0.3838 | 
INFO:root:| Training Epoch 27:: | | Average Loss: 1.7923, | | Average Accuracy: 0.5170, | | Avg Grad Norm: 17.4805, | | Samples: 528 | | CE Loss: 1.4089 | | Distill Loss: 1.7646 | | Lt, Ls Loss: 0.3917, 0.3997 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 27:: | | Average Loss: 1.8027, | | Average Accuracy: 0.5287, | | Samples: 87 | | CE Loss: 1.4211 | | Distill Loss: 1.7937 | | Lt, Ls Loss: 0.3600, 0.3901 | 
INFO:root:| Training Epoch 28:: | | Average Loss: 1.7860, | | Average Accuracy: 0.5739, | | Avg Grad Norm: 17.3069, | | Samples: 528 | | CE Loss: 1.3841 | | Distill Loss: 1.8141 | | Lt, Ls Loss: 0.4029, 0.4622 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 28:: | | Average Loss: 1.7922, | | Average Accuracy: 0.4943, | | Samples: 87 | | CE Loss: 1.4196 | | Distill Loss: 1.8327 | | Lt, Ls Loss: 0.2905, 0.3610 | 
INFO:root:| Training Epoch 29:: | | Average Loss: 1.7512, | | Average Accuracy: 0.6080, | | Avg Grad Norm: 20.2407, | | Samples: 528 | | CE Loss: 1.3688 | | Distill Loss: 1.8318 | | Lt, Ls Loss: 0.3353, 0.3827 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 29:: | | Average Loss: 1.8053, | | Average Accuracy: 0.5172, | | Samples: 87 | | CE Loss: 1.4263 | | Distill Loss: 1.8200 | | Lt, Ls Loss: 0.3405, 0.3659 | 
INFO:root:| Training Epoch 30:: | | Average Loss: 1.7479, | | Average Accuracy: 0.6269, | | Avg Grad Norm: 26.0907, | | Samples: 528 | | CE Loss: 1.3664 | | Distill Loss: 1.8066 | | Lt, Ls Loss: 0.3512, 0.3856 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 30:: | | Average Loss: 1.7823, | | Average Accuracy: 0.5172, | | Samples: 87 | | CE Loss: 1.4154 | | Distill Loss: 1.8144 | | Lt, Ls Loss: 0.3181, 0.3131 | 
INFO:root:| Training Epoch 31:: | | Average Loss: 1.7090, | | Average Accuracy: 0.6307, | | Avg Grad Norm: 21.0571, | | Samples: 528 | | CE Loss: 1.3331 | | Distill Loss: 1.8404 | | Lt, Ls Loss: 0.3199, 0.3459 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 31:: | | Average Loss: 1.7396, | | Average Accuracy: 0.5517, | | Samples: 87 | | CE Loss: 1.3733 | | Distill Loss: 1.8640 | | Lt, Ls Loss: 0.3057, 0.2724 | 
INFO:root:| Training Epoch 32:: | | Average Loss: 1.6711, | | Average Accuracy: 0.6117, | | Avg Grad Norm: 17.9479, | | Samples: 528 | | CE Loss: 1.3059 | | Distill Loss: 1.8570 | | Lt, Ls Loss: 0.2875, 0.2901 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 32:: | | Average Loss: 1.7067, | | Average Accuracy: 0.5172, | | Samples: 87 | | CE Loss: 1.3492 | | Distill Loss: 1.8637 | | Lt, Ls Loss: 0.2606, 0.2590 | 
INFO:root:| Training Epoch 33:: | | Average Loss: 1.6594, | | Average Accuracy: 0.6080, | | Avg Grad Norm: 21.4812, | | Samples: 528 | | CE Loss: 1.2908 | | Distill Loss: 1.8478 | | Lt, Ls Loss: 0.2919, 0.3174 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 33:: | | Average Loss: 1.7279, | | Average Accuracy: 0.5632, | | Samples: 87 | | CE Loss: 1.3600 | | Distill Loss: 1.8452 | | Lt, Ls Loss: 0.2801, 0.3272 | 
INFO:root:| Training Epoch 34:: | | Average Loss: 1.6436, | | Average Accuracy: 0.6004, | | Avg Grad Norm: 43.6569, | | Samples: 528 | | CE Loss: 1.2793 | | Distill Loss: 1.8358 | | Lt, Ls Loss: 0.2809, 0.3115 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 34:: | | Average Loss: 1.6846, | | Average Accuracy: 0.5172, | | Samples: 87 | | CE Loss: 1.3341 | | Distill Loss: 1.8564 | | Lt, Ls Loss: 0.2320, 0.2482 | 
INFO:root:| Training Epoch 35:: | | Average Loss: 1.6344, | | Average Accuracy: 0.6212, | | Avg Grad Norm: 19.8060, | | Samples: 528 | | CE Loss: 1.2708 | | Distill Loss: 1.8535 | | Lt, Ls Loss: 0.2766, 0.2940 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 35:: | | Average Loss: 1.6862, | | Average Accuracy: 0.5517, | | Samples: 87 | | CE Loss: 1.3204 | | Distill Loss: 1.8566 | | Lt, Ls Loss: 0.2803, 0.3021 | 
INFO:root:| Training Epoch 36:: | | Average Loss: 1.6215, | | Average Accuracy: 0.6420, | | Avg Grad Norm: 26.1911, | | Samples: 528 | | CE Loss: 1.2568 | | Distill Loss: 1.8419 | | Lt, Ls Loss: 0.2859, 0.3039 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 36:: | | Average Loss: 1.6699, | | Average Accuracy: 0.5402, | | Samples: 87 | | CE Loss: 1.3182 | | Distill Loss: 1.8346 | | Lt, Ls Loss: 0.2573, 0.2524 | 
INFO:root:| Training Epoch 37:: | | Average Loss: 1.6112, | | Average Accuracy: 0.6534, | | Avg Grad Norm: 21.0864, | | Samples: 528 | | CE Loss: 1.2463 | | Distill Loss: 1.8540 | | Lt, Ls Loss: 0.2814, 0.2973 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 37:: | | Average Loss: 1.6702, | | Average Accuracy: 0.5287, | | Samples: 87 | | CE Loss: 1.2907 | | Distill Loss: 1.8967 | | Lt, Ls Loss: 0.3095, 0.3238 | 
INFO:root:| Training Epoch 38:: | | Average Loss: 1.6006, | | Average Accuracy: 0.6364, | | Avg Grad Norm: 18.3654, | | Samples: 528 | | CE Loss: 1.2275 | | Distill Loss: 1.9084 | | Lt, Ls Loss: 0.2677, 0.3117 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 38:: | | Average Loss: 1.6556, | | Average Accuracy: 0.5287, | | Samples: 87 | | CE Loss: 1.2813 | | Distill Loss: 1.9297 | | Lt, Ls Loss: 0.2641, 0.3017 | 
INFO:root:| Training Epoch 39:: | | Average Loss: 1.5872, | | Average Accuracy: 0.6383, | | Avg Grad Norm: 36.3649, | | Samples: 528 | | CE Loss: 1.2100 | | Distill Loss: 1.9328 | | Lt, Ls Loss: 0.2872, 0.2946 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 39:: | | Average Loss: 1.6701, | | Average Accuracy: 0.5517, | | Samples: 87 | | CE Loss: 1.2725 | | Distill Loss: 1.9484 | | Lt, Ls Loss: 0.3593, 0.3430 | 
INFO:root:| Training Epoch 40:: | | Average Loss: 1.5849, | | Average Accuracy: 0.6496, | | Avg Grad Norm: 21.4384, | | Samples: 528 | | CE Loss: 1.2053 | | Distill Loss: 1.9330 | | Lt, Ls Loss: 0.2938, 0.3039 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 40:: | | Average Loss: 1.6649, | | Average Accuracy: 0.5862, | | Samples: 87 | | CE Loss: 1.2852 | | Distill Loss: 1.9190 | | Lt, Ls Loss: 0.3067, 0.3060 | 
INFO:root:| Training Epoch 41:: | | Average Loss: 1.5989, | | Average Accuracy: 0.6496, | | Avg Grad Norm: 25.6013, | | Samples: 528 | | CE Loss: 1.2140 | | Distill Loss: 1.8996 | | Lt, Ls Loss: 0.3266, 0.3399 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 41:: | | Average Loss: 1.6761, | | Average Accuracy: 0.5862, | | Samples: 87 | | CE Loss: 1.2908 | | Distill Loss: 1.8909 | | Lt, Ls Loss: 0.3410, 0.3365 | 
INFO:root:| Training Epoch 42:: | | Average Loss: 1.5945, | | Average Accuracy: 0.6591, | | Avg Grad Norm: 21.8607, | | Samples: 528 | | CE Loss: 1.2152 | | Distill Loss: 1.8786 | | Lt, Ls Loss: 0.3234, 0.3267 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 42:: | | Average Loss: 1.6511, | | Average Accuracy: 0.5977, | | Samples: 87 | | CE Loss: 1.2811 | | Distill Loss: 1.8783 | | Lt, Ls Loss: 0.2870, 0.3012 | 
INFO:root:| Training Epoch 43:: | | Average Loss: 1.5825, | | Average Accuracy: 0.6667, | | Avg Grad Norm: 35.0833, | | Samples: 528 | | CE Loss: 1.2085 | | Distill Loss: 1.8668 | | Lt, Ls Loss: 0.3120, 0.3141 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 43:: | | Average Loss: 1.6534, | | Average Accuracy: 0.5632, | | Samples: 87 | | CE Loss: 1.2764 | | Distill Loss: 1.8738 | | Lt, Ls Loss: 0.3072, 0.3319 | 
INFO:root:| Training Epoch 44:: | | Average Loss: 1.5717, | | Average Accuracy: 0.6610, | | Avg Grad Norm: 35.3667, | | Samples: 528 | | CE Loss: 1.2040 | | Distill Loss: 1.8694 | | Lt, Ls Loss: 0.2808, 0.3014 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 44:: | | Average Loss: 1.6359, | | Average Accuracy: 0.5862, | | Samples: 87 | | CE Loss: 1.2703 | | Distill Loss: 1.8802 | | Lt, Ls Loss: 0.2603, 0.2973 | 
INFO:root:| Training Epoch 45:: | | Average Loss: 1.5665, | | Average Accuracy: 0.6629, | | Avg Grad Norm: 26.7866, | | Samples: 528 | | CE Loss: 1.1998 | | Distill Loss: 1.8727 | | Lt, Ls Loss: 0.2793, 0.2926 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 45:: | | Average Loss: 1.6415, | | Average Accuracy: 0.5862, | | Samples: 87 | | CE Loss: 1.2692 | | Distill Loss: 1.8797 | | Lt, Ls Loss: 0.2806, 0.3215 | 
INFO:root:| Training Epoch 46:: | | Average Loss: 1.5631, | | Average Accuracy: 0.6610, | | Avg Grad Norm: 23.2178, | | Samples: 528 | | CE Loss: 1.1977 | | Distill Loss: 1.8740 | | Lt, Ls Loss: 0.2764, 0.2858 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 46:: | | Average Loss: 1.6396, | | Average Accuracy: 0.5747, | | Samples: 87 | | CE Loss: 1.2664 | | Distill Loss: 1.8866 | | Lt, Ls Loss: 0.3152, 0.2857 | 
INFO:root:| Training Epoch 47:: | | Average Loss: 1.5590, | | Average Accuracy: 0.6648, | | Avg Grad Norm: 57.4719, | | Samples: 528 | | CE Loss: 1.1948 | | Distill Loss: 1.8797 | | Lt, Ls Loss: 0.2718, 0.2769 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 47:: | | Average Loss: 1.6410, | | Average Accuracy: 0.5747, | | Samples: 87 | | CE Loss: 1.2661 | | Distill Loss: 1.8890 | | Lt, Ls Loss: 0.3140, 0.2961 | 
INFO:root:| Training Epoch 48:: | | Average Loss: 1.5539, | | Average Accuracy: 0.6667, | | Avg Grad Norm: 15.0816, | | Samples: 528 | | CE Loss: 1.1932 | | Distill Loss: 1.8822 | | Lt, Ls Loss: 0.2656, 0.2574 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 48:: | | Average Loss: 1.6373, | | Average Accuracy: 0.5747, | | Samples: 87 | | CE Loss: 1.2649 | | Distill Loss: 1.8914 | | Lt, Ls Loss: 0.3009, 0.2901 | 
INFO:root:| Training Epoch 49:: | | Average Loss: 1.5528, | | Average Accuracy: 0.6705, | | Avg Grad Norm: 18.0571, | | Samples: 528 | | CE Loss: 1.1921 | | Distill Loss: 1.8837 | | Lt, Ls Loss: 0.2607, 0.2602 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 49:: | | Average Loss: 1.6335, | | Average Accuracy: 0.5747, | | Samples: 87 | | CE Loss: 1.2645 | | Distill Loss: 1.8921 | | Lt, Ls Loss: 0.2860, 0.2819 | 
INFO:root:| Evaluation Epoch 49:: | | Average Loss: 1.6627, | | Average Accuracy: 0.6154, | | Samples: 91 | | CE Loss: 1.3053 | | Distill Loss: 1.8643 | | Lt, Ls Loss: 0.2489, 0.2692 | 
INFO:root:Test Performance: Average Loss: 1.6627, Average Accuracy: 0.6154, 
INFO:root:

Starting Fine-tuning for task 1.

INFO:root:| Training Epoch 0:: | | Average Loss: 6.0173, | | Average Accuracy: 0.0393, | | Avg Grad Norm: 102.0313, | | Samples: 305 | | CE Loss: 5.6512 | | Distill Loss: 1.8241 | | Lt, Ls Loss: 0.3008, 0.3160 | | Max Grad Limit: 1.5 | 
INFO:root:| Training Epoch 1:: | | Average Loss: 5.9873, | | Average Accuracy: 0.0393, | | Avg Grad Norm: 118.2959, | | Samples: 305 | | CE Loss: 5.6311 | | Distill Loss: 1.8110 | | Lt, Ls Loss: 0.2814, 0.2822 | | Max Grad Limit: 1.5 | 
INFO:root:| Training Epoch 2:: | | Average Loss: 5.9788, | | Average Accuracy: 0.0393, | | Avg Grad Norm: 110.2755, | | Samples: 305 | | CE Loss: 5.6200 | | Distill Loss: 1.8005 | | Lt, Ls Loss: 0.2862, 0.3048 | | Max Grad Limit: 1.5 | 
INFO:root:| Training Epoch 3:: | | Average Loss: 5.8720, | | Average Accuracy: 0.0393, | | Avg Grad Norm: 71.1417, | | Samples: 305 | | CE Loss: 5.5237 | | Distill Loss: 1.7188 | | Lt, Ls Loss: 0.3056, 0.2976 | | Max Grad Limit: 1.5 | 
INFO:root:| Training Epoch 4:: | | Average Loss: 5.5817, | | Average Accuracy: 0.0393, | | Avg Grad Norm: 88.5401, | | Samples: 305 | | CE Loss: 5.2690 | | Distill Loss: 1.4481 | | Lt, Ls Loss: 0.3178, 0.3184 | | Max Grad Limit: 1.5 | 
INFO:root:| Training Epoch 5:: | | Average Loss: 5.1818, | | Average Accuracy: 0.0361, | | Avg Grad Norm: 152.2256, | | Samples: 305 | | CE Loss: 4.9397 | | Distill Loss: 0.9977 | | Lt, Ls Loss: 0.3035, 0.3122 | | Max Grad Limit: 1.5 | 
INFO:root:| Training Epoch 6:: | | Average Loss: 4.8753, | | Average Accuracy: 0.0590, | | Avg Grad Norm: 97.5839, | | Samples: 305 | | CE Loss: 4.6694 | | Distill Loss: 0.6106 | | Lt, Ls Loss: 0.3820, 0.3798 | | Max Grad Limit: 1.5 | 
INFO:root:| Training Epoch 7:: | | Average Loss: 4.5583, | | Average Accuracy: 0.0525, | | Avg Grad Norm: 71.4934, | | Samples: 305 | | CE Loss: 4.3819 | | Distill Loss: 0.4354 | | Lt, Ls Loss: 0.3775, 0.3628 | | Max Grad Limit: 1.5 | 
INFO:root:| Training Epoch 8:: | | Average Loss: 4.3267, | | Average Accuracy: 0.0820, | | Avg Grad Norm: 97.3494, | | Samples: 305 | | CE Loss: 4.1844 | | Distill Loss: 0.3246 | | Lt, Ls Loss: 0.3176, 0.3065 | | Max Grad Limit: 1.5 | 
INFO:root:| Training Epoch 9:: | | Average Loss: 4.1548, | | Average Accuracy: 0.0984, | | Avg Grad Norm: 118.1492, | | Samples: 305 | | CE Loss: 3.9836 | | Distill Loss: 0.3049 | | Lt, Ls Loss: 0.4399, 0.3966 | | Max Grad Limit: 1.5 | 
INFO:root:| Training Epoch 10:: | | Average Loss: 3.9860, | | Average Accuracy: 0.1082, | | Avg Grad Norm: 116.3861, | | Samples: 305 | | CE Loss: 3.8169 | | Distill Loss: 0.3008 | | Lt, Ls Loss: 0.4215, 0.4051 | | Max Grad Limit: 1.5 | 
INFO:root:| Training Epoch 11:: | | Average Loss: 3.8076, | | Average Accuracy: 0.1410, | | Avg Grad Norm: 105.2114, | | Samples: 305 | | CE Loss: 3.6486 | | Distill Loss: 0.2757 | | Lt, Ls Loss: 0.3919, 0.3924 | | Max Grad Limit: 1.5 | 
INFO:root:| Training Epoch 12:: | | Average Loss: 3.9213, | | Average Accuracy: 0.1279, | | Avg Grad Norm: 136.2153, | | Samples: 305 | | CE Loss: 3.7786 | | Distill Loss: 0.2258 | | Lt, Ls Loss: 0.3616, 0.3642 | | Max Grad Limit: 1.5 | 
INFO:root:| Training Epoch 13:: | | Average Loss: 3.7739, | | Average Accuracy: 0.1344, | | Avg Grad Norm: 112.1449, | | Samples: 305 | | CE Loss: 3.6394 | | Distill Loss: 0.1857 | | Lt, Ls Loss: 0.3583, 0.3529 | | Max Grad Limit: 1.5 | 
INFO:root:| Training Epoch 14:: | | Average Loss: 3.9125, | | Average Accuracy: 0.1344, | | Avg Grad Norm: 179.9304, | | Samples: 305 | | CE Loss: 3.7835 | | Distill Loss: 0.1687 | | Lt, Ls Loss: 0.3529, 0.3379 | | Max Grad Limit: 1.5 | 
