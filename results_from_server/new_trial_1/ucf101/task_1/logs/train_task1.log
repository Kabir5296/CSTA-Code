INFO:root:

Training for task 1 starting on: 08/12/2025, 22:41:02

INFO:root:The config is being used from file: config/train_configs/UCF101/train_task1_unfreeze_ft.yml
INFO:root:
Starting with calculating features and relations for distillation purposes.


INFO:root:Architecture already matches Task 0
INFO:root:Model architecture prepared: 0 adapter(s), 1 classifier(s).
INFO:root:

DO NOT WORRY ABOUT THE CHECKPOINT LOADING. 
We're now loading the actual checkpoint from model_save/new_trial_1/ucf101/task_0/model/checkpoints/final_model.pth
INFO:root:Loading weights from: model_save/new_trial_1/ucf101/task_0/model/checkpoints/final_model.pth
INFO:root:Extracting features and relations for memory bank...
INFO:root:Saved Spatial Features: model_save/new_trial_1/bank/task_0_model/spatial_feats_task0.pt (torch.Size([471, 512]))
INFO:root:Saved Temporal Features: model_save/new_trial_1/bank/task_0_model/temporal_feats_task0.pt (torch.Size([471, 512]))
INFO:root:Saved Spatial Relations: model_save/new_trial_1/bank/task_0_model/spatial_relations_task0.json (torch.Size([471, 52]))
INFO:root:Saved Temporal Relations: model_save/new_trial_1/bank/task_0_model/temporal_relations_task0.json (torch.Size([471, 52]))
INFO:root:Old features and relations bank created successfully, now starting current task: 1.


INFO:root:Task 1: Loading checkpoint from Task 0 at model_save/new_trial_1/ucf101/task_0/model/checkpoints/final_model.pth
INFO:root:Loading weights from: model_save/new_trial_1/ucf101/task_0/model/checkpoints/final_model.pth
INFO:root:Task 1: Freezing all parameters except for the new components.
INFO:root:Model architecture prepared: 1 adapter(s), 2 classifier(s).
INFO:root:Model config: model_name: csta, dim: 512, num_heads: 8, num_layers: 8


INFO:root:| Training Epoch 0:: | | Average Loss: 5.3721, | | Average Accuracy: 0.0000, | | Avg Grad Norm: 491.0655, | | Samples: 471 | | CE Loss: 5.1699 | | Distill Loss: 0.4136 | | Lt, Ls Loss: 0.4549, 0.4793 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 0:: | | Average Loss: 5.0592, | | Average Accuracy: 0.0000, | | Samples: 79 | | CE Loss: 4.8695 | | Distill Loss: 0.3127 | | Lt, Ls Loss: 0.4528, 0.4997 | 
INFO:root:| Training Epoch 1:: | | Average Loss: 4.7516, | | Average Accuracy: 0.0000, | | Avg Grad Norm: 228.4279, | | Samples: 471 | | CE Loss: 4.5549 | | Distill Loss: 0.3010 | | Lt, Ls Loss: 0.4922, 0.5182 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 1:: | | Average Loss: 4.4701, | | Average Accuracy: 0.0000, | | Samples: 79 | | CE Loss: 4.2680 | | Distill Loss: 0.3040 | | Lt, Ls Loss: 0.5093, 0.5340 | 
INFO:root:| Training Epoch 2:: | | Average Loss: 4.2330, | | Average Accuracy: 0.0000, | | Avg Grad Norm: 252.0915, | | Samples: 471 | | CE Loss: 4.0512 | | Distill Loss: 0.3252 | | Lt, Ls Loss: 0.4227, 0.4640 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 2:: | | Average Loss: 4.0669, | | Average Accuracy: 0.0000, | | Samples: 79 | | CE Loss: 3.8654 | | Distill Loss: 0.3519 | | Lt, Ls Loss: 0.4636, 0.5278 | 
INFO:root:| Training Epoch 3:: | | Average Loss: 3.8431, | | Average Accuracy: 0.0085, | | Avg Grad Norm: 381.3955, | | Samples: 471 | | CE Loss: 3.6362 | | Distill Loss: 0.3765 | | Lt, Ls Loss: 0.4926, 0.5106 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 3:: | | Average Loss: 3.7260, | | Average Accuracy: 0.0253, | | Samples: 79 | | CE Loss: 3.5085 | | Distill Loss: 0.3764 | | Lt, Ls Loss: 0.5103, 0.5630 | 
INFO:root:| Training Epoch 4:: | | Average Loss: 3.5161, | | Average Accuracy: 0.0425, | | Avg Grad Norm: 181.8425, | | Samples: 471 | | CE Loss: 3.2983 | | Distill Loss: 0.3877 | | Lt, Ls Loss: 0.5114, 0.5528 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 4:: | | Average Loss: 3.4299, | | Average Accuracy: 0.0759, | | Samples: 79 | | CE Loss: 3.2205 | | Distill Loss: 0.3837 | | Lt, Ls Loss: 0.4646, 0.5479 | 
INFO:root:| Training Epoch 5:: | | Average Loss: 3.3044, | | Average Accuracy: 0.0828, | | Avg Grad Norm: 166.2131, | | Samples: 471 | | CE Loss: 3.1053 | | Distill Loss: 0.4095 | | Lt, Ls Loss: 0.4172, 0.5006 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 5:: | | Average Loss: 3.3097, | | Average Accuracy: 0.0506, | | Samples: 79 | | CE Loss: 3.0995 | | Distill Loss: 0.4330 | | Lt, Ls Loss: 0.4676, 0.5003 | 
INFO:root:| Training Epoch 6:: | | Average Loss: 3.1394, | | Average Accuracy: 0.1062, | | Avg Grad Norm: 232.0546, | | Samples: 471 | | CE Loss: 2.9348 | | Distill Loss: 0.4561 | | Lt, Ls Loss: 0.4269, 0.4814 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 6:: | | Average Loss: 3.0464, | | Average Accuracy: 0.1646, | | Samples: 79 | | CE Loss: 2.8379 | | Distill Loss: 0.4582 | | Lt, Ls Loss: 0.4489, 0.4829 | 
INFO:root:| Training Epoch 7:: | | Average Loss: 2.8765, | | Average Accuracy: 0.1953, | | Avg Grad Norm: 225.1576, | | Samples: 471 | | CE Loss: 2.6725 | | Distill Loss: 0.4603 | | Lt, Ls Loss: 0.4397, 0.4598 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 7:: | | Average Loss: 2.8370, | | Average Accuracy: 0.1266, | | Samples: 79 | | CE Loss: 2.6312 | | Distill Loss: 0.4307 | | Lt, Ls Loss: 0.4284, 0.5129 | 
INFO:root:| Training Epoch 8:: | | Average Loss: 2.6832, | | Average Accuracy: 0.2569, | | Avg Grad Norm: 171.2279, | | Samples: 471 | | CE Loss: 2.4730 | | Distill Loss: 0.4596 | | Lt, Ls Loss: 0.4592, 0.4827 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 8:: | | Average Loss: 2.6672, | | Average Accuracy: 0.2405, | | Samples: 79 | | CE Loss: 2.4646 | | Distill Loss: 0.4536 | | Lt, Ls Loss: 0.4540, 0.4429 | 
INFO:root:| Training Epoch 9:: | | Average Loss: 2.6134, | | Average Accuracy: 0.2972, | | Avg Grad Norm: 166.0749, | | Samples: 471 | | CE Loss: 2.4023 | | Distill Loss: 0.4971 | | Lt, Ls Loss: 0.4282, 0.4818 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 9:: | | Average Loss: 2.7087, | | Average Accuracy: 0.2532, | | Samples: 79 | | CE Loss: 2.4893 | | Distill Loss: 0.4729 | | Lt, Ls Loss: 0.4539, 0.5363 | 
INFO:root:| Training Epoch 10:: | | Average Loss: 2.5406, | | Average Accuracy: 0.3206, | | Avg Grad Norm: 166.8319, | | Samples: 471 | | CE Loss: 2.3244 | | Distill Loss: 0.5226 | | Lt, Ls Loss: 0.4368, 0.4815 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 10:: | | Average Loss: 2.5192, | | Average Accuracy: 0.3291, | | Samples: 79 | | CE Loss: 2.2960 | | Distill Loss: 0.4974 | | Lt, Ls Loss: 0.4711, 0.5194 | 
INFO:root:| Training Epoch 11:: | | Average Loss: 2.3954, | | Average Accuracy: 0.3949, | | Avg Grad Norm: 148.6755, | | Samples: 471 | | CE Loss: 2.1661 | | Distill Loss: 0.5145 | | Lt, Ls Loss: 0.4935, 0.5208 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 11:: | | Average Loss: 2.4371, | | Average Accuracy: 0.3038, | | Samples: 79 | | CE Loss: 2.1964 | | Distill Loss: 0.5043 | | Lt, Ls Loss: 0.5493, 0.5515 | 
INFO:root:| Training Epoch 12:: | | Average Loss: 2.3169, | | Average Accuracy: 0.4268, | | Avg Grad Norm: 285.1368, | | Samples: 471 | | CE Loss: 2.0778 | | Distill Loss: 0.5500 | | Lt, Ls Loss: 0.5009, 0.5434 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 12:: | | Average Loss: 2.3837, | | Average Accuracy: 0.3165, | | Samples: 79 | | CE Loss: 2.1429 | | Distill Loss: 0.5442 | | Lt, Ls Loss: 0.5132, 0.5481 | 
INFO:root:| Training Epoch 13:: | | Average Loss: 2.2651, | | Average Accuracy: 0.4225, | | Avg Grad Norm: 121.0884, | | Samples: 471 | | CE Loss: 2.0329 | | Distill Loss: 0.5656 | | Lt, Ls Loss: 0.4895, 0.4930 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 13:: | | Average Loss: 2.3390, | | Average Accuracy: 0.3038, | | Samples: 79 | | CE Loss: 2.1166 | | Distill Loss: 0.5526 | | Lt, Ls Loss: 0.4648, 0.4656 | 
INFO:root:| Training Epoch 14:: | | Average Loss: 2.2097, | | Average Accuracy: 0.4034, | | Avg Grad Norm: 154.8680, | | Samples: 471 | | CE Loss: 1.9957 | | Distill Loss: 0.5639 | | Lt, Ls Loss: 0.4222, 0.4408 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 14:: | | Average Loss: 2.2302, | | Average Accuracy: 0.3418, | | Samples: 79 | | CE Loss: 2.0163 | | Distill Loss: 0.5270 | | Lt, Ls Loss: 0.4312, 0.4680 | 
INFO:root:| Training Epoch 15:: | | Average Loss: 2.1495, | | Average Accuracy: 0.4544, | | Avg Grad Norm: 142.2473, | | Samples: 471 | | CE Loss: 1.9252 | | Distill Loss: 0.5416 | | Lt, Ls Loss: 0.4551, 0.4985 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 15:: | | Average Loss: 2.2315, | | Average Accuracy: 0.2785, | | Samples: 79 | | CE Loss: 1.9989 | | Distill Loss: 0.5563 | | Lt, Ls Loss: 0.5086, 0.4857 | 
INFO:root:| Training Epoch 16:: | | Average Loss: 2.1056, | | Average Accuracy: 0.4246, | | Avg Grad Norm: 229.9109, | | Samples: 471 | | CE Loss: 1.8773 | | Distill Loss: 0.5886 | | Lt, Ls Loss: 0.4652, 0.4682 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 16:: | | Average Loss: 2.1691, | | Average Accuracy: 0.2785, | | Samples: 79 | | CE Loss: 1.9338 | | Distill Loss: 0.5758 | | Lt, Ls Loss: 0.4939, 0.4993 | 
INFO:root:| Training Epoch 17:: | | Average Loss: 2.0406, | | Average Accuracy: 0.4331, | | Avg Grad Norm: 223.1843, | | Samples: 471 | | CE Loss: 1.8172 | | Distill Loss: 0.5914 | | Lt, Ls Loss: 0.4337, 0.4645 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 17:: | | Average Loss: 2.1309, | | Average Accuracy: 0.3418, | | Samples: 79 | | CE Loss: 1.8948 | | Distill Loss: 0.5720 | | Lt, Ls Loss: 0.5178, 0.4844 | 
INFO:root:| Training Epoch 18:: | | Average Loss: 2.0319, | | Average Accuracy: 0.4352, | | Avg Grad Norm: 211.7761, | | Samples: 471 | | CE Loss: 1.8015 | | Distill Loss: 0.5975 | | Lt, Ls Loss: 0.4463, 0.4922 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 18:: | | Average Loss: 2.1559, | | Average Accuracy: 0.3418, | | Samples: 79 | | CE Loss: 1.9019 | | Distill Loss: 0.6061 | | Lt, Ls Loss: 0.5319, 0.5550 | 
INFO:root:| Training Epoch 19:: | | Average Loss: 2.0563, | | Average Accuracy: 0.4416, | | Avg Grad Norm: 167.7125, | | Samples: 471 | | CE Loss: 1.8091 | | Distill Loss: 0.6350 | | Lt, Ls Loss: 0.4918, 0.5209 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 19:: | | Average Loss: 2.1018, | | Average Accuracy: 0.4177, | | Samples: 79 | | CE Loss: 1.8739 | | Distill Loss: 0.6442 | | Lt, Ls Loss: 0.4364, 0.4388 | 
INFO:root:| Training Epoch 20:: | | Average Loss: 2.0159, | | Average Accuracy: 0.4586, | | Avg Grad Norm: 270.7156, | | Samples: 471 | | CE Loss: 1.7765 | | Distill Loss: 0.6688 | | Lt, Ls Loss: 0.4465, 0.4803 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 20:: | | Average Loss: 2.0909, | | Average Accuracy: 0.4430, | | Samples: 79 | | CE Loss: 1.8389 | | Distill Loss: 0.6835 | | Lt, Ls Loss: 0.4437, 0.5529 | 
INFO:root:| Training Epoch 21:: | | Average Loss: 2.0224, | | Average Accuracy: 0.4671, | | Avg Grad Norm: 166.0631, | | Samples: 471 | | CE Loss: 1.7690 | | Distill Loss: 0.7542 | | Lt, Ls Loss: 0.4460, 0.4885 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 21:: | | Average Loss: 2.0985, | | Average Accuracy: 0.4051, | | Samples: 79 | | CE Loss: 1.8497 | | Distill Loss: 0.7697 | | Lt, Ls Loss: 0.3853, 0.5034 | 
INFO:root:| Training Epoch 22:: | | Average Loss: 1.9995, | | Average Accuracy: 0.4416, | | Avg Grad Norm: 181.7260, | | Samples: 471 | | CE Loss: 1.7581 | | Distill Loss: 0.7738 | | Lt, Ls Loss: 0.4115, 0.4243 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 22:: | | Average Loss: 2.0488, | | Average Accuracy: 0.3797, | | Samples: 79 | | CE Loss: 1.8154 | | Distill Loss: 0.7106 | | Lt, Ls Loss: 0.4131, 0.4319 | 
INFO:root:| Training Epoch 23:: | | Average Loss: 1.9631, | | Average Accuracy: 0.4628, | | Avg Grad Norm: 206.7394, | | Samples: 471 | | CE Loss: 1.7185 | | Distill Loss: 0.7157 | | Lt, Ls Loss: 0.4467, 0.4683 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 23:: | | Average Loss: 2.0160, | | Average Accuracy: 0.4557, | | Samples: 79 | | CE Loss: 1.7796 | | Distill Loss: 0.6952 | | Lt, Ls Loss: 0.4224, 0.4582 | 
INFO:root:| Training Epoch 24:: | | Average Loss: 1.9453, | | Average Accuracy: 0.4841, | | Avg Grad Norm: 165.8836, | | Samples: 471 | | CE Loss: 1.6903 | | Distill Loss: 0.6862 | | Lt, Ls Loss: 0.4879, 0.5259 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 24:: | | Average Loss: 1.9805, | | Average Accuracy: 0.4304, | | Samples: 79 | | CE Loss: 1.7562 | | Distill Loss: 0.6502 | | Lt, Ls Loss: 0.3966, 0.4486 | 
INFO:root:| Training Epoch 25:: | | Average Loss: 1.9032, | | Average Accuracy: 0.4883, | | Avg Grad Norm: 170.3328, | | Samples: 471 | | CE Loss: 1.6641 | | Distill Loss: 0.6525 | | Lt, Ls Loss: 0.4271, 0.5142 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 25:: | | Average Loss: 1.9522, | | Average Accuracy: 0.4177, | | Samples: 79 | | CE Loss: 1.7377 | | Distill Loss: 0.6375 | | Lt, Ls Loss: 0.3362, 0.4561 | 
INFO:root:| Training Epoch 26:: | | Average Loss: 1.8872, | | Average Accuracy: 0.4904, | | Avg Grad Norm: 187.6236, | | Samples: 471 | | CE Loss: 1.6446 | | Distill Loss: 0.6693 | | Lt, Ls Loss: 0.4582, 0.4898 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 26:: | | Average Loss: 1.9533, | | Average Accuracy: 0.4430, | | Samples: 79 | | CE Loss: 1.7282 | | Distill Loss: 0.6907 | | Lt, Ls Loss: 0.3729, 0.4371 | 
INFO:root:| Training Epoch 27:: | | Average Loss: 1.8718, | | Average Accuracy: 0.4989, | | Avg Grad Norm: 159.1693, | | Samples: 471 | | CE Loss: 1.6285 | | Distill Loss: 0.7181 | | Lt, Ls Loss: 0.4213, 0.4820 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 27:: | | Average Loss: 1.9570, | | Average Accuracy: 0.4430, | | Samples: 79 | | CE Loss: 1.7235 | | Distill Loss: 0.7340 | | Lt, Ls Loss: 0.3638, 0.4588 | 
INFO:root:| Training Epoch 28:: | | Average Loss: 1.8659, | | Average Accuracy: 0.5117, | | Avg Grad Norm: 102.2309, | | Samples: 471 | | CE Loss: 1.6285 | | Distill Loss: 0.7775 | | Lt, Ls Loss: 0.3885, 0.4165 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 28:: | | Average Loss: 1.9882, | | Average Accuracy: 0.4177, | | Samples: 79 | | CE Loss: 1.7306 | | Distill Loss: 0.7788 | | Lt, Ls Loss: 0.4208, 0.5180 | 
INFO:root:| Training Epoch 29:: | | Average Loss: 1.8612, | | Average Accuracy: 0.5032, | | Avg Grad Norm: 127.8526, | | Samples: 471 | | CE Loss: 1.6202 | | Distill Loss: 0.8041 | | Lt, Ls Loss: 0.3746, 0.4282 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 29:: | | Average Loss: 1.9740, | | Average Accuracy: 0.4304, | | Samples: 79 | | CE Loss: 1.7060 | | Distill Loss: 0.7913 | | Lt, Ls Loss: 0.4792, 0.5165 | 
INFO:root:| Training Epoch 30:: | | Average Loss: 1.8440, | | Average Accuracy: 0.4989, | | Avg Grad Norm: 158.8139, | | Samples: 471 | | CE Loss: 1.5973 | | Distill Loss: 0.8183 | | Lt, Ls Loss: 0.3893, 0.4370 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 30:: | | Average Loss: 1.9403, | | Average Accuracy: 0.4177, | | Samples: 79 | | CE Loss: 1.6944 | | Distill Loss: 0.8016 | | Lt, Ls Loss: 0.4013, 0.4366 | 
INFO:root:| Training Epoch 31:: | | Average Loss: 1.8359, | | Average Accuracy: 0.5096, | | Avg Grad Norm: 134.5840, | | Samples: 471 | | CE Loss: 1.5839 | | Distill Loss: 0.8151 | | Lt, Ls Loss: 0.4047, 0.4604 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 31:: | | Average Loss: 1.9525, | | Average Accuracy: 0.4304, | | Samples: 79 | | CE Loss: 1.6861 | | Distill Loss: 0.7891 | | Lt, Ls Loss: 0.4430, 0.5444 | 
INFO:root:| Training Epoch 32:: | | Average Loss: 1.8240, | | Average Accuracy: 0.5180, | | Avg Grad Norm: 168.5114, | | Samples: 471 | | CE Loss: 1.5786 | | Distill Loss: 0.8008 | | Lt, Ls Loss: 0.3905, 0.4448 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 32:: | | Average Loss: 1.9469, | | Average Accuracy: 0.4177, | | Samples: 79 | | CE Loss: 1.6826 | | Distill Loss: 0.7908 | | Lt, Ls Loss: 0.4768, 0.4944 | 
INFO:root:| Training Epoch 33:: | | Average Loss: 1.8224, | | Average Accuracy: 0.5244, | | Avg Grad Norm: 183.8832, | | Samples: 471 | | CE Loss: 1.5731 | | Distill Loss: 0.7983 | | Lt, Ls Loss: 0.4147, 0.4493 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 33:: | | Average Loss: 1.9360, | | Average Accuracy: 0.4304, | | Samples: 79 | | CE Loss: 1.6756 | | Distill Loss: 0.7767 | | Lt, Ls Loss: 0.4509, 0.5080 | 
INFO:root:| Training Epoch 34:: | | Average Loss: 1.8206, | | Average Accuracy: 0.5244, | | Avg Grad Norm: 104.7498, | | Samples: 471 | | CE Loss: 1.5699 | | Distill Loss: 0.7885 | | Lt, Ls Loss: 0.4084, 0.4745 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 34:: | | Average Loss: 1.9314, | | Average Accuracy: 0.4177, | | Samples: 79 | | CE Loss: 1.6794 | | Distill Loss: 0.7668 | | Lt, Ls Loss: 0.4290, 0.4843 | 
INFO:root:| Training Epoch 35:: | | Average Loss: 1.8142, | | Average Accuracy: 0.5223, | | Avg Grad Norm: 131.2694, | | Samples: 471 | | CE Loss: 1.5651 | | Distill Loss: 0.7723 | | Lt, Ls Loss: 0.4077, 0.4804 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 35:: | | Average Loss: 1.9441, | | Average Accuracy: 0.4684, | | Samples: 79 | | CE Loss: 1.6724 | | Distill Loss: 0.7568 | | Lt, Ls Loss: 0.4830, 0.5716 | 
INFO:root:| Training Epoch 36:: | | Average Loss: 1.8080, | | Average Accuracy: 0.5372, | | Avg Grad Norm: 205.6442, | | Samples: 471 | | CE Loss: 1.5592 | | Distill Loss: 0.7660 | | Lt, Ls Loss: 0.4154, 0.4776 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 36:: | | Average Loss: 1.9063, | | Average Accuracy: 0.4430, | | Samples: 79 | | CE Loss: 1.6671 | | Distill Loss: 0.7629 | | Lt, Ls Loss: 0.3577, 0.4740 | 
INFO:root:| Training Epoch 37:: | | Average Loss: 1.7955, | | Average Accuracy: 0.5265, | | Avg Grad Norm: 131.5880, | | Samples: 471 | | CE Loss: 1.5526 | | Distill Loss: 0.7767 | | Lt, Ls Loss: 0.3904, 0.4527 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 37:: | | Average Loss: 1.8988, | | Average Accuracy: 0.4304, | | Samples: 79 | | CE Loss: 1.6618 | | Distill Loss: 0.7701 | | Lt, Ls Loss: 0.4020, 0.4081 | 
INFO:root:| Training Epoch 38:: | | Average Loss: 1.7899, | | Average Accuracy: 0.5265, | | Avg Grad Norm: 111.6460, | | Samples: 471 | | CE Loss: 1.5477 | | Distill Loss: 0.7879 | | Lt, Ls Loss: 0.3915, 0.4354 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 38:: | | Average Loss: 1.8972, | | Average Accuracy: 0.4304, | | Samples: 79 | | CE Loss: 1.6557 | | Distill Loss: 0.7739 | | Lt, Ls Loss: 0.4069, 0.4291 | 
INFO:root:| Training Epoch 39:: | | Average Loss: 1.7806, | | Average Accuracy: 0.5308, | | Avg Grad Norm: 145.5227, | | Samples: 471 | | CE Loss: 1.5383 | | Distill Loss: 0.7859 | | Lt, Ls Loss: 0.3855, 0.4435 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 39:: | | Average Loss: 1.8817, | | Average Accuracy: 0.4430, | | Samples: 79 | | CE Loss: 1.6456 | | Distill Loss: 0.7696 | | Lt, Ls Loss: 0.4078, 0.3965 | 
INFO:root:| Training Epoch 40:: | | Average Loss: 1.7739, | | Average Accuracy: 0.5308, | | Avg Grad Norm: 142.4043, | | Samples: 471 | | CE Loss: 1.5304 | | Distill Loss: 0.7819 | | Lt, Ls Loss: 0.3990, 0.4424 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 40:: | | Average Loss: 1.8768, | | Average Accuracy: 0.4304, | | Samples: 79 | | CE Loss: 1.6426 | | Distill Loss: 0.7698 | | Lt, Ls Loss: 0.3904, 0.4010 | 
INFO:root:| Training Epoch 41:: | | Average Loss: 1.7788, | | Average Accuracy: 0.5287, | | Avg Grad Norm: 229.2532, | | Samples: 471 | | CE Loss: 1.5266 | | Distill Loss: 0.7858 | | Lt, Ls Loss: 0.4194, 0.4761 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 41:: | | Average Loss: 1.8856, | | Average Accuracy: 0.4430, | | Samples: 79 | | CE Loss: 1.6404 | | Distill Loss: 0.7792 | | Lt, Ls Loss: 0.4049, 0.4508 | 
INFO:root:| Training Epoch 42:: | | Average Loss: 1.7789, | | Average Accuracy: 0.5265, | | Avg Grad Norm: 134.2893, | | Samples: 471 | | CE Loss: 1.5276 | | Distill Loss: 0.7970 | | Lt, Ls Loss: 0.4178, 0.4610 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 42:: | | Average Loss: 1.8832, | | Average Accuracy: 0.4430, | | Samples: 79 | | CE Loss: 1.6406 | | Distill Loss: 0.7881 | | Lt, Ls Loss: 0.3865, 0.4425 | 
INFO:root:| Training Epoch 43:: | | Average Loss: 1.7710, | | Average Accuracy: 0.5180, | | Avg Grad Norm: 144.9038, | | Samples: 471 | | CE Loss: 1.5288 | | Distill Loss: 0.8040 | | Lt, Ls Loss: 0.3829, 0.4275 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 43:: | | Average Loss: 1.8763, | | Average Accuracy: 0.4304, | | Samples: 79 | | CE Loss: 1.6395 | | Distill Loss: 0.7918 | | Lt, Ls Loss: 0.3750, 0.4113 | 
INFO:root:| Training Epoch 44:: | | Average Loss: 1.7675, | | Average Accuracy: 0.5159, | | Avg Grad Norm: 154.3711, | | Samples: 471 | | CE Loss: 1.5266 | | Distill Loss: 0.8083 | | Lt, Ls Loss: 0.3804, 0.4172 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 44:: | | Average Loss: 1.8808, | | Average Accuracy: 0.4304, | | Samples: 79 | | CE Loss: 1.6376 | | Distill Loss: 0.7939 | | Lt, Ls Loss: 0.3858, 0.4420 | 
INFO:root:| Training Epoch 45:: | | Average Loss: 1.7671, | | Average Accuracy: 0.5244, | | Avg Grad Norm: 125.8689, | | Samples: 471 | | CE Loss: 1.5257 | | Distill Loss: 0.8075 | | Lt, Ls Loss: 0.3774, 0.4247 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 45:: | | Average Loss: 1.8806, | | Average Accuracy: 0.4304, | | Samples: 79 | | CE Loss: 1.6366 | | Distill Loss: 0.7925 | | Lt, Ls Loss: 0.3868, 0.4476 | 
INFO:root:| Training Epoch 46:: | | Average Loss: 1.7622, | | Average Accuracy: 0.5244, | | Avg Grad Norm: 178.6765, | | Samples: 471 | | CE Loss: 1.5244 | | Distill Loss: 0.8069 | | Lt, Ls Loss: 0.3673, 0.4113 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 46:: | | Average Loss: 1.8703, | | Average Accuracy: 0.4304, | | Samples: 79 | | CE Loss: 1.6347 | | Distill Loss: 0.7931 | | Lt, Ls Loss: 0.3679, 0.4096 | 
INFO:root:| Training Epoch 47:: | | Average Loss: 1.7566, | | Average Accuracy: 0.5223, | | Avg Grad Norm: 155.1525, | | Samples: 471 | | CE Loss: 1.5239 | | Distill Loss: 0.8078 | | Lt, Ls Loss: 0.3505, 0.3935 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 47:: | | Average Loss: 1.8723, | | Average Accuracy: 0.4304, | | Samples: 79 | | CE Loss: 1.6341 | | Distill Loss: 0.7934 | | Lt, Ls Loss: 0.3610, 0.4336 | 
INFO:root:| Training Epoch 48:: | | Average Loss: 1.7543, | | Average Accuracy: 0.5223, | | Avg Grad Norm: 104.4968, | | Samples: 471 | | CE Loss: 1.5238 | | Distill Loss: 0.8076 | | Lt, Ls Loss: 0.3459, 0.3832 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 48:: | | Average Loss: 1.8755, | | Average Accuracy: 0.4304, | | Samples: 79 | | CE Loss: 1.6336 | | Distill Loss: 0.7934 | | Lt, Ls Loss: 0.3844, 0.4352 | 
INFO:root:| Training Epoch 49:: | | Average Loss: 1.7516, | | Average Accuracy: 0.5223, | | Avg Grad Norm: 108.3099, | | Samples: 471 | | CE Loss: 1.5237 | | Distill Loss: 0.8078 | | Lt, Ls Loss: 0.3353, 0.3764 | | Max Grad Limit: 1.5 | 
INFO:root:| Evaluation Epoch 49:: | | Average Loss: 1.8744, | | Average Accuracy: 0.4304, | | Samples: 79 | | CE Loss: 1.6335 | | Distill Loss: 0.7935 | | Lt, Ls Loss: 0.3870, 0.4258 | 
INFO:root:| Evaluation Epoch 49:: | | Average Loss: 1.8761, | | Average Accuracy: 0.4500, | | Samples: 80 | | CE Loss: 1.6302 | | Distill Loss: 0.7853 | | Lt, Ls Loss: 0.3896, 0.4644 | 
INFO:root:Test Performance: Average Loss: 1.8761, Average Accuracy: 0.4500, 
INFO:root:

Starting Fine-tuning for task 0.

INFO:root:| Training Epoch 0:: | | Average Loss: 4.2795, | | Average Accuracy: 0.0857, | | Avg Grad Norm: 417.8640, | | Samples: 280 | | CE Loss: 4.0280 | | Distill Loss: 0.8321 | | Lt, Ls Loss: 0.4012, 0.4436 | | Max Grad Limit: 1.5 | 
INFO:root:| Training Epoch 1:: | | Average Loss: 4.2686, | | Average Accuracy: 0.0857, | | Avg Grad Norm: 288.8444, | | Samples: 280 | | CE Loss: 4.0251 | | Distill Loss: 0.8332 | | Lt, Ls Loss: 0.3737, 0.4163 | | Max Grad Limit: 1.5 | 
INFO:root:| Training Epoch 2:: | | Average Loss: 4.2648, | | Average Accuracy: 0.0857, | | Avg Grad Norm: 307.1361, | | Samples: 280 | | CE Loss: 4.0216 | | Distill Loss: 0.8338 | | Lt, Ls Loss: 0.3712, 0.4160 | | Max Grad Limit: 1.5 | 
INFO:root:| Training Epoch 3:: | | Average Loss: 4.2505, | | Average Accuracy: 0.0857, | | Avg Grad Norm: 387.2407, | | Samples: 280 | | CE Loss: 4.0067 | | Distill Loss: 0.8382 | | Lt, Ls Loss: 0.3692, 0.4177 | | Max Grad Limit: 1.5 | 
INFO:root:| Training Epoch 4:: | | Average Loss: 4.2062, | | Average Accuracy: 0.0893, | | Avg Grad Norm: 390.9862, | | Samples: 280 | | CE Loss: 3.9625 | | Distill Loss: 0.8443 | | Lt, Ls Loss: 0.3660, 0.4143 | | Max Grad Limit: 1.5 | 
INFO:root:| Training Epoch 5:: | | Average Loss: 4.1297, | | Average Accuracy: 0.1036, | | Avg Grad Norm: 311.1856, | | Samples: 280 | | CE Loss: 3.8728 | | Distill Loss: 0.8595 | | Lt, Ls Loss: 0.3988, 0.4543 | | Max Grad Limit: 1.5 | 
INFO:root:| Training Epoch 6:: | | Average Loss: 3.9900, | | Average Accuracy: 0.1071, | | Avg Grad Norm: 759.0496, | | Samples: 280 | | CE Loss: 3.7214 | | Distill Loss: 0.8693 | | Lt, Ls Loss: 0.4469, 0.4748 | | Max Grad Limit: 1.5 | 
INFO:root:| Training Epoch 7:: | | Average Loss: 3.7908, | | Average Accuracy: 0.1286, | | Avg Grad Norm: 454.2727, | | Samples: 280 | | CE Loss: 3.5320 | | Distill Loss: 0.8610 | | Lt, Ls Loss: 0.4068, 0.4574 | | Max Grad Limit: 1.5 | 
INFO:root:| Training Epoch 8:: | | Average Loss: 3.5335, | | Average Accuracy: 0.1857, | | Avg Grad Norm: 293.6162, | | Samples: 280 | | CE Loss: 3.2770 | | Distill Loss: 0.8358 | | Lt, Ls Loss: 0.4364, 0.4377 | | Max Grad Limit: 1.5 | 
INFO:root:| Training Epoch 9:: | | Average Loss: 3.1774, | | Average Accuracy: 0.2536, | | Avg Grad Norm: 306.7093, | | Samples: 280 | | CE Loss: 2.9255 | | Distill Loss: 0.7979 | | Lt, Ls Loss: 0.4282, 0.4534 | | Max Grad Limit: 1.5 | 
